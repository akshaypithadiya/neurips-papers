[
    {
        "abstract": "Abstract Missing",
        "authors": "Misha Ahrens, Maneesh Sahani",
        "pdf": "http://papers.nips.cc/paper/3224-inferring-elapsed-time-from-stochastic-neural-processes.pdf",
        "title": "Inferring Elapsed Time from Stochastic Neural Processes"
    },
    {
        "abstract": "We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm, proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems.",
        "authors": "András Antos, Csaba Szepesvári, Rémi Munos",
        "pdf": "http://papers.nips.cc/paper/3233-fitted-q-iteration-in-continuous-action-space-mdps.pdf",
        "title": "Fitted Q-iteration in continuous action-space MDPs"
    },
    {
        "abstract": "Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multi-modal. We propose a variational treatment of diffusion processes, which allows us to estimate these parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. Furthermore, our parameter inference scheme does not break down when the time step gets smaller, unlike most current approaches. Finally, we show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy.",
        "authors": "Cédric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, John S. Shawe-taylor",
        "pdf": "http://papers.nips.cc/paper/3282-variational-inference-for-diffusion-processes.pdf",
        "title": "Variational Inference for Diffusion Processes"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andreas Argyriou, Massimiliano Pontil, Yiming Ying, Charles A. Micchelli",
        "pdf": "http://papers.nips.cc/paper/3187-a-spectral-regularization-framework-for-multi-task-structure-learning.pdf",
        "title": "A Spectral Regularization Framework for Multi-Task Structure Learning"
    },
    {
        "abstract": "We combine two threads of research on approximate dynamic programming: random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function. This combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states. Our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics. In this paper we show that we can now solve problems we couldn't solve previously with regular grid-based approaches.",
        "authors": "Chris Atkeson, Benjamin Stephens",
        "pdf": "http://papers.nips.cc/paper/3350-random-sampling-of-states-in-dynamic-programming.pdf",
        "title": "Random Sampling of States in Dynamic Programming"
    },
    {
        "abstract": "We consider the learning task consisting in predicting as well as the best function in a finite reference set G up to the smallest possible additive term. If R(g) denotes the generalization error of a prediction function g, under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded), it is known that the progressive mixture rule g_n satisfies E R(g_n) < min_{g in G} R(g) + Cst (log|G|)/n where n denotes the size of the training set, E denotes the expectation wrt the training set distribution. This work shows that, surprisingly, for appropriate reference sets G, the deviation convergence rate of the progressive mixture rule is only no better than Cst / sqrt{n}, and not the expected Cst / n. It also provides an algorithm which does not suffer from this drawback.",
        "authors": "Jean-yves Audibert",
        "pdf": "http://papers.nips.cc/paper/3303-progressive-mixture-rules-are-deviation-suboptimal.pdf",
        "title": "Progressive mixture rules are deviation suboptimal"
    },
    {
        "abstract": "We present a novel linear clustering framework (Diffrac) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem. The large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions. This framework has several attractive properties: (1) although apparently similar to K-means, it exhibits superior clustering performance than K-means, in particular in terms of robustness to noise. (2) It can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels, and can then be seen as an alternative to spectral clustering. (3) Prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classification. We present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.",
        "authors": "Francis R. Bach, Zaïd Harchaoui",
        "pdf": "http://papers.nips.cc/paper/3269-diffrac-a-discriminative-and-flexible-framework-for-clustering.pdf",
        "title": "DIFFRAC: a discriminative and flexible framework for clustering"
    },
    {
        "abstract": "We present a new analysis for the combination of binary classifiers. We propose a theoretical framework based on the Neyman-Pearson lemma to analyze combinations of classifiers. In particular, we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal ROC curve. We also show how our method generalizes and improves on previous work on combining classifiers and generating ROC curves.",
        "authors": "Marco Barreno, Alvaro Cardenas, J. D. Tygar",
        "pdf": "http://papers.nips.cc/paper/3263-optimal-roc-curve-for-a-combination-of-classifiers.pdf",
        "title": "Optimal ROC Curve for a Combination of Classifiers"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Elad Hazan, Alexander Rakhlin, Peter L. Bartlett",
        "pdf": "http://papers.nips.cc/paper/3319-adaptive-online-gradient-descent.pdf",
        "title": "Adaptive Online Gradient Descent"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Zafer Barutcuoglu, Phil Long, Rocco Servedio",
        "pdf": "http://papers.nips.cc/paper/3238-one-pass-boosting.pdf",
        "title": "One-Pass Boosting"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ulrik Beierholm, Ladan Shams, Wei J. Ma, Konrad Koerding",
        "pdf": "http://papers.nips.cc/paper/3207-comparing-bayesian-models-for-multisensory-cue-combination-without-mandatory-integration.pdf",
        "title": "Comparing Bayesian models for multisensory cue combination without mandatory integration"
    },
    {
        "abstract": "Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we find that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly overcomplete.",
        "authors": "Pietro Berkes, Richard Turner, Maneesh Sahani",
        "pdf": "http://papers.nips.cc/paper/3312-on-sparsity-and-overcompleteness-in-image-models.pdf",
        "title": "On Sparsity and Overcompleteness in Image Models"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Matthias Bethge, Philipp Berens",
        "pdf": "http://papers.nips.cc/paper/3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf",
        "title": "Near-Maximum Entropy Models for Binary Neural Representations of Natural Images"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, Richard S. Sutton",
        "pdf": "http://papers.nips.cc/paper/3258-incremental-natural-actor-critic-algorithms.pdf",
        "title": "Incremental Natural Actor-Critic Algorithms"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Benjamin Blankertz, Motoaki Kawanabe, Ryota Tomioka, Friederike Hohlefeld, Klaus-Robert Müller, Vadim V. Nikulin",
        "pdf": "http://papers.nips.cc/paper/3184-invariant-common-spatial-patterns-alleviating-nonstationarities-in-brain-computer-interfacing.pdf",
        "title": "Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing"
    },
    {
        "abstract": "We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.",
        "authors": "Jon D. Mcauliffe, David M. Blei",
        "pdf": "http://papers.nips.cc/paper/3328-supervised-topic-models.pdf",
        "title": "Supervised Topic Models"
    },
    {
        "abstract": "Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.",
        "authors": "John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman",
        "pdf": "http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation.pdf",
        "title": "Learning Bounds for Domain Adaptation"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ben Blum, David Baker, Michael I. Jordan, Philip Bradley, Rhiju Das, David E. Kim",
        "pdf": "http://papers.nips.cc/paper/3173-feature-selection-methods-for-improving-protein-structure-prediction-with-rosetta.pdf",
        "title": "Feature Selection Methods for Improving Protein Structure Prediction with Rosetta"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Omer Bobrowski, Ron Meir, Shy Shoham, Yonina Eldar",
        "pdf": "http://papers.nips.cc/paper/3299-a-neural-network-implementing-optimal-state-estimation-based-on-dynamic-spike-train-decoding.pdf",
        "title": "A neural network implementing optimal state estimation based on dynamic spike train decoding"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Edwin V. Bonilla, Kian M. Chai, Christopher Williams",
        "pdf": "http://papers.nips.cc/paper/3189-multi-task-gaussian-process-prediction.pdf",
        "title": "Multi-task Gaussian Process Prediction"
    },
    {
        "abstract": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation--estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.",
        "authors": "Léon Bottou, Olivier Bousquet",
        "pdf": "http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf",
        "title": "The Tradeoffs of Large Scale Learning"
    },
    {
        "abstract": "We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. Our framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change, evaluating these schemes using the reconstruction of ancient word forms in Romance languages. The result is an efficient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies.",
        "authors": "Alexandre Bouchard-côté, Percy S. Liang, Dan Klein, Thomas L. Griffiths",
        "pdf": "http://papers.nips.cc/paper/3196-a-probabilistic-approach-to-language-change.pdf",
        "title": "A Probabilistic Approach to Language Change"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Sabri Boutemedjet, Djemel Ziou, Nizar Bouguila",
        "pdf": "http://papers.nips.cc/paper/3267-unsupervised-feature-selection-for-accurate-recommendation-of-high-dimensional-image-data.pdf",
        "title": "Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Joseph K. Bradley, Robert E. Schapire",
        "pdf": "http://papers.nips.cc/paper/3321-filterboost-regression-and-classification-on-large-datasets.pdf",
        "title": "FilterBoost: Regression and Classification on Large Datasets"
    },
    {
        "abstract": "We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \\cite{KlampflETAL:07b}. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis {(PCA)} with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals X that are related or are not related to some additional target signal Y_T. In a biological interpretation, this target signal Y_T (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.",
        "authors": "Lars Buesing, Wolfgang Maass",
        "pdf": "http://papers.nips.cc/paper/3168-simplified-rules-and-theoretical-analysis-for-information-bottleneck-optimization-and-pca-with-spiking-neurons.pdf",
        "title": "Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons"
    },
    {
        "abstract": "Assessing similarity between features is a key step in object recognition and scene categorization tasks. We argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not. Intuitively one would expect that similarities between features could arise from any distribution. In this paper, we will derive the contrary, and report the theoretical result that L_p-norms --a class of commonly applied distance metrics-- from one feature vector to other vectors are Weibull-distributed if the feature values are correlated and non-identically distributed. Besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images. This fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms.\r\n\r\nErratum: The authors of paper have declared that they have become convinced that the reasoning in the reference is too simple as a proof of their claims. As a consequence, they withdraw their theorems.",
        "authors": "Gertjan Burghouts, Arnold Smeulders, Jan-mark Geusebroek",
        "pdf": "http://papers.nips.cc/paper/3367-the-distribution-family-of-similarity-distances.pdf",
        "title": "The Distribution Family of Similarity Distances"
    },
    {
        "abstract": "Many tasks in speech processing involve classification of long term characteristics of a speech segment such as language, speaker, dialect, or topic. A natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words, phones, etc. From these tokens, we can then look for distinctive phrases, keywords, that characterize the speech. In many applications, a set of distinctive keywords may not be known a priori. In this case, an automatic method of building up keywords from short context units such as phones is desirable. We propose a method for construction of keywords based upon Support Vector Machines. We cast the problem of keyword selection as a feature selection problem for n-grams of phones. We propose an alternating filter-wrapper method that builds successively longer keywords. Application of this method on a language recognition task shows that the technique produces interesting and significant qualitative and quantitative results.",
        "authors": "Fred Richardson, William M. Campbell",
        "pdf": "http://papers.nips.cc/paper/3272-discriminative-keyword-selection-using-support-vector-machines.pdf",
        "title": "Discriminative Keyword Selection Using Support Vector Machines"
    },
    {
        "abstract": "We propose a model that leverages the millions of clicks received by web search engines, to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.",
        "authors": "Ben Carterette, Rosie Jones",
        "pdf": "http://papers.nips.cc/paper/3190-evaluating-search-engines-by-modeling-the-relationship-between-relevance-and-clicks.pdf",
        "title": "Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Gonzalo Carvajal, Waldo Valenzuela, Miguel Figueroa",
        "pdf": "http://papers.nips.cc/paper/3261-subspace-based-face-recognition-in-analog-vlsi.pdf",
        "title": "Subspace-Based Face Recognition in Analog VLSI"
    },
    {
        "abstract": "Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.",
        "authors": "Lawrence Cayton, Sanjoy Dasgupta",
        "pdf": "http://papers.nips.cc/paper/3357-a-learning-framework-for-nearest-neighbor-search.pdf",
        "title": "A learning framework for nearest neighbor search"
    },
    {
        "abstract": "Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models have aimed at predicting such voluntary attentional shifts. Although the importance of high level stimulus properties (higher order statistics, semantics) stands undisputed, most models are based on low-level features of the input alone. In this study we recorded eye-movements of human observers while they viewed photographs of natural scenes. About two thirds of the stimuli contained at least one person. We demonstrate that a combined model of face detection and low-level saliency clearly outperforms a low-level model in predicting locations humans fixate. This is reflected in our finding fact that observes, even when not instructed to look for anything particular, fixate on a face with a probability of over 80% within their first two fixations (500ms). Remarkably, the model's predictive performance in images that do not contain faces is not impaired by spurious face detector responses, which is suggestive of a bottom-up mechanism for face detection. In summary, we provide a novel computational approach which combines high level object knowledge (in our case: face locations) with low-level features to successfully predict the allocation of attentional resources.",
        "authors": "Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch",
        "pdf": "http://papers.nips.cc/paper/3169-predicting-human-gaze-using-low-level-saliency-combined-with-face-detection.pdf",
        "title": "Predicting human gaze using low-level saliency combined with face detection"
    },
    {
        "abstract": "We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.",
        "authors": "Venkat Chandrasekaran, Alan S. Willsky, Jason K. Johnson",
        "pdf": "http://papers.nips.cc/paper/3275-adaptive-embedded-subgraph-algorithms-using-walk-sum-analysis.pdf",
        "title": "Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis"
    },
    {
        "abstract": "Support Vector Machines (SVMs) suffer from a widely recognized scalability problem in both memory use and computational time. To improve scalability, we have developed a parallel SVM algorithm (PSVM), which reduces memory use through performing a row-based, approximate matrix factorization, and which loads only essential data to each machine to perform parallel computation. Let n denote the number of training instances, p the reduced matrix dimension after factorization (p is significantly smaller than n), and m the number of machines. PSVM reduces the memory requirement from \\MO(n^2) to \\MO(np/m), and improves computation time to \\MO(np^2/m). Empirical studies on up to 500 computers shows PSVM to be effective.",
        "authors": "Kaihua Zhu, Hao Wang, Hongjie Bai, Jian Li, Zhihuan Qiu, Hang Cui, Edward Y. Chang",
        "pdf": "http://papers.nips.cc/paper/3202-parallelizing-support-vector-machines-on-distributed-computers.pdf",
        "title": "Parallelizing Support Vector Machines on Distributed Computers"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Nicolas Chapados, Yoshua Bengio",
        "pdf": "http://papers.nips.cc/paper/3324-augmented-functional-time-series-representation-and-forecasting-with-gaussian-processes.pdf",
        "title": "Augmented Functional Time Series Representation and Forecasting with Gaussian Processes"
    },
    {
        "abstract": "We present the first truly polynomial algorithm for learning the structure of bounded-treewidth junction trees -- an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity, and provides strong theoretical guarantees in terms of KL divergence from the true distribution. We also present a lazy extension of our approach that leads to very significant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of random variables with only a polynomial number of mutual information computations on fixed-size subsets of variables, when the underlying distribution can be approximated by a bounded treewidth junction tree.",
        "authors": "Anton Chechetka, Carlos Guestrin",
        "pdf": "http://papers.nips.cc/paper/3164-efficient-principled-learning-of-thin-junction-trees.pdf",
        "title": "Efficient Principled Learning of Thin Junction Trees"
    },
    {
        "abstract": "Semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning. In this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals. Our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training. Comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer. We discuss relevant issues and relate our regularizer to previous work.",
        "authors": "Ke Chen, Shihai Wang",
        "pdf": "http://papers.nips.cc/paper/3167-regularized-boost-for-semi-supervised-learning.pdf",
        "title": "Regularized Boost for Semi-Supervised Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Yuanhao Chen, Long Zhu, Chenxi Lin, Hongjiang Zhang, Alan L. Yuille",
        "pdf": "http://papers.nips.cc/paper/3327-rapid-inference-on-a-novel-andor-graph-for-object-detection-segmentation-and-parsing.pdf",
        "title": "Rapid Inference on a Novel AND/OR graph for Object Detection, Segmentation and Parsing"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Hai L. Chieu, Wee S. Lee, Yee W. Teh",
        "pdf": "http://papers.nips.cc/paper/3308-cooled-and-relaxed-survey-propagation-for-mrfs.pdf",
        "title": "Cooled and Relaxed Survey Propagation for MRFs"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andreas Christmann, Ingo Steinwart",
        "pdf": "http://papers.nips.cc/paper/3180-how-svms-can-estimate-quantiles-and-the-median.pdf",
        "title": "How SVMs can estimate quantiles and the median"
    },
    {
        "abstract": "Traditional analysis methods for single-trial classification of electro-encephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classification, i.e. event related potentials; and second order methods, in which the feature of interest is the power of the signal, i.e event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by knowledge of neurological findings. Here we propose a unified method in which the algorithm learns the best first and second order spatial and temporal features for classification of EEG based on a bilinear model. The efficiency of the method is demonstrated in simulated and real EEG from a benchmark data set for Brain Computer Interface.",
        "authors": "Christoforos Christoforou, Paul Sajda, Lucas C. Parra",
        "pdf": "http://papers.nips.cc/paper/3236-second-order-bilinear-discriminant-analysis-for-single-trial-eeg-analysis.pdf",
        "title": "Second Order Bilinear Discriminant Analysis for single trial EEG analysis"
    },
    {
        "abstract": "Independent component analysis (ICA) is a powerful method to decouple signals. Most of the algorithms performing ICA do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution. Moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations. In this paper, we are interested in understanding the neural mechanism responsible for solving ICA. We present an online learning rule that exploits delayed correlations in the input. This rule performs ICA by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based Hebbian learning rule.",
        "authors": "Claudia Clopath, André Longtin, Wulfram Gerstner",
        "pdf": "http://papers.nips.cc/paper/3174-an-online-hebbian-learning-rule-that-performs-independent-component-analysis.pdf",
        "title": "An online Hebbian learning rule that performs Independent Component Analysis"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "John P. Cunningham, Byron M. Yu, Krishna V. Shenoy, Maneesh Sahani",
        "pdf": "http://papers.nips.cc/paper/3229-inferring-neural-firing-rates-from-spike-trains-using-gaussian-processes.pdf",
        "title": "Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Pierre Dangauthier, Ralf Herbrich, Tom Minka, Thore Graepel",
        "pdf": "http://papers.nips.cc/paper/3331-trueskill-through-time-revisiting-the-history-of-chess.pdf",
        "title": "TrueSkill Through Time: Revisiting the History of Chess"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Varsha Dani, Sham M. Kakade, Thomas P. Hayes",
        "pdf": "http://papers.nips.cc/paper/3371-the-price-of-bandit-information-for-online-optimization.pdf",
        "title": "The Price of Bandit Information for Online Optimization"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Sanjoy Dasgupta, Daniel J. Hsu, Claire Monteleoni",
        "pdf": "http://papers.nips.cc/paper/3325-a-general-agnostic-active-learning-algorithm.pdf",
        "title": "A general agnostic active learning algorithm"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Justin Dauwels, François Vialatte, Tomasz Rutkowski, Andrzej S. Cichocki",
        "pdf": "http://papers.nips.cc/paper/3322-measuring-neural-synchrony-by-message-passing.pdf",
        "title": "Measuring Neural Synchrony by Message Passing"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Aaron C. Courville, Nathaniel D. Daw",
        "pdf": "http://papers.nips.cc/paper/3205-the-rat-as-particle-filter.pdf",
        "title": "The rat as particle filter"
    },
    {
        "abstract": "Using multiple regularization hyperparameters is an effective method for managing model complexity in problems where input features have varying amounts of noise. While algorithms for choosing multiple hyperparameters are often used in neural networks and support vector machines, they are not common in structured prediction tasks, such as sequence labeling or parsing. In this paper, we consider the problem of learning regularization hyperparameters for log-linear models, a class of probabilistic models for structured prediction tasks which includes conditional random fields (CRFs). Using an implicit differentiation trick, we derive an efficient gradient-based method for learning Gaussian regularization priors with multiple hyperparameters. In both simulations and the real-world task of computational RNA secondary structure prediction, we find that multiple hyperparameter learning provides a significant boost in accuracy compared to models learned using only a single regularization hyperparameter.",
        "authors": "Chuan-sheng Foo, Chuong B. Do, Andrew Y. Ng",
        "pdf": "http://papers.nips.cc/paper/3286-efficient-multiple-hyperparameter-learning-for-log-linear-models.pdf",
        "title": "Efficient multiple hyperparameter learning for log-linear models"
    },
    {
        "abstract": "Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of Web2.0\" recommender systems, allowing users to generate playlists based on use-dependent terms such as \"chill\" or \"jogging\" that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of boosted classifiers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or \"autotags\") furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the ''cold-start problem'' common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.\"",
        "authors": "Douglas Eck, Paul Lamere, Thierry Bertin-mahieux, Stephen Green",
        "pdf": "http://papers.nips.cc/paper/3370-automatic-generation-of-social-tags-for-music-recommendation.pdf",
        "title": "Automatic Generation of Social Tags for Music Recommendation"
    },
    {
        "abstract": "The peristimulus time historgram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spiketrains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin with or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation \\cite{ShimazakiBinningNIPS2006,ShimazakiBinningNECO2007}. We develop an exact Bayesian, generative model approach to estimating PSHTs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions.",
        "authors": "Dominik Endres, Mike Oram, Johannes Schindelin, Peter Foldiak",
        "pdf": "http://papers.nips.cc/paper/3216-bayesian-binning-beats-approximate-alternatives-estimating-peri-stimulus-time-histograms.pdf",
        "title": "Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Gwenn Englebienne, Tim Cootes, Magnus Rattray",
        "pdf": "http://papers.nips.cc/paper/3287-a-probabilistic-model-for-generating-realistic-lip-movements-from-speech.pdf",
        "title": "A probabilistic model for generating realistic lip movements from speech"
    },
    {
        "abstract": "We propose an active learning algorithm that learns a continuous valuation model from discrete preferences. The algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden. To do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive. The problem is particularly difficult because the space of choices is infinite. We demonstrate the effectiveness of the new algorithm compared to related active learning methods. We also embed the algorithm within a decision making tool for assisting digital artists in rendering materials. The tool finds the best parameters while minimizing the number of queries.",
        "authors": "Brochu Eric, Nando D. Freitas, Abhijeet Ghosh",
        "pdf": "http://papers.nips.cc/paper/3219-active-preference-learning-with-discrete-choice-data.pdf",
        "title": "Active Preference Learning with Discrete Choice Data"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Tim V. Erven, Steven D. Rooij, Peter Grünwald",
        "pdf": "http://papers.nips.cc/paper/3277-catching-up-faster-in-bayesian-model-selection-and-model-averaging.pdf",
        "title": "Catching Up Faster in Bayesian Model Selection and Model Averaging"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Saher Esmeir, Shaul Markovitch",
        "pdf": "http://papers.nips.cc/paper/3192-anytime-induction-of-cost-sensitive-trees.pdf",
        "title": "Anytime Induction of Cost-sensitive Trees"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Vittorio Ferrari, Andrew Zisserman",
        "pdf": "http://papers.nips.cc/paper/3217-learning-visual-attributes.pdf",
        "title": "Learning Visual Attributes"
    },
    {
        "abstract": "Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment confirms the previously reported presence of a new kind of ErrP. These Interaction ErrP\" exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak (~290, ~350 and ~470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject's intent while trying to mentally drive the cursor of 73.1%. These results show that it's possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the brain-computer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex.\"",
        "authors": "Pierre Ferrez, José Millán",
        "pdf": "http://papers.nips.cc/paper/3377-eeg-based-brain-computer-interaction-improved-accuracy-by-automatic-single-trial-error-detection.pdf",
        "title": "EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection"
    },
    {
        "abstract": "Sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates. While the matching models can explain properties of neural responses, no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization near the center of gaze. Here, we examine two models for the barn owl's sound localization behavior. First, we consider a maximum likelihood estimator in order to further evaluate the cue matching model. Second, we consider a maximum a posteriori estimator to test if a Bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owl's localization behavior. We show that the maximum likelihood estimator can not reproduce the owl's behavior, while the maximum a posteriori estimator is able to match the behavior. This result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl. The Bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owl's localization behavior.",
        "authors": "Brian J. Fischer",
        "pdf": "http://papers.nips.cc/paper/3244-optimal-models-of-sound-localization-by-barn-owls.pdf",
        "title": "Optimal models of sound localization by barn owls"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Noah Goodman, Joshua B. Tenenbaum, Michael J. Black",
        "pdf": "http://papers.nips.cc/paper/3165-a-bayesian-framework-for-cross-situational-word-learning.pdf",
        "title": "A Bayesian Framework for Cross-Situational Word-Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Peter Frazier, Angela J. Yu",
        "pdf": "http://papers.nips.cc/paper/3355-sequential-hypothesis-testing-under-stochastic-deadlines.pdf",
        "title": "Sequential Hypothesis Testing under Stochastic Deadlines"
    },
    {
        "abstract": "We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data.",
        "authors": "Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, Nakul Verma",
        "pdf": "http://papers.nips.cc/paper/3195-learning-the-structure-of-manifolds-using-random-projections.pdf",
        "title": "Learning the structure of manifolds using random projections"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Charlie Frogner, Avi Pfeffer",
        "pdf": "http://papers.nips.cc/paper/3223-discovering-weakly-interacting-factors-in-a-complex-stochastic-process.pdf",
        "title": "Discovering Weakly-Interacting Factors in a Complex Stochastic Process"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, Bernhard Schölkopf",
        "pdf": "http://papers.nips.cc/paper/3340-kernel-measures-of-conditional-dependence.pdf",
        "title": "Kernel Measures of Conditional Dependence"
    },
    {
        "abstract": "The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion fields, and even dynamic textures), and applied to a number of the latter (the prediction of human eye fixations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye fixations better than previous models, and produce background subtraction algorithms that outperform the state-of-the-art in computer vision.",
        "authors": "Dashan Gao, Vijay Mahadevan, Nuno Vasconcelos",
        "pdf": "http://papers.nips.cc/paper/3264-the-discriminant-center-surround-hypothesis-for-bottom-up-saliency.pdf",
        "title": "The discriminant center-surround hypothesis for bottom-up saliency"
    },
    {
        "abstract": "It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive fields. However, the resulting sparse coefficients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1, and we discuss the implications of our findings for physiological experiments.",
        "authors": "Pierre Garrigues, Bruno A. Olshausen",
        "pdf": "http://papers.nips.cc/paper/3237-learning-horizontal-connections-in-a-sparse-coding-model-of-natural-images.pdf",
        "title": "Learning Horizontal Connections in a Sparse Coding Model of Natural Images"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Michael Gashler, Dan Ventura, Tony Martinez",
        "pdf": "http://papers.nips.cc/paper/3241-iterative-non-linear-dimensionality-reduction-with-manifold-sculpting.pdf",
        "title": "Iterative Non-linear Dimensionality Reduction with Manifold Sculpting"
    },
    {
        "abstract": "A new algorithm for on-line learning linear-threshold functions is proposed which efficiently combines second-order statistics about the data with the logarithmic behavior\" of multiplicative/dual-norm algorithms. An initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard Perceptron algorithm operating on a transformed sequence of examples with improved margin properties. We also report on experiments carried out on datasets from diverse domains, with the goal of comparing to known Perceptron algorithms (first-order, second-order, additive, multiplicative). Our learning procedure seems to generalize quite well, and converges faster than the corresponding multiplicative baseline algorithms.\"",
        "authors": "Claudio Gentile, Fabio Vitale, Cristian Brotto",
        "pdf": "http://papers.nips.cc/paper/3209-on-higher-order-perceptron-algorithms.pdf",
        "title": "On higher-order perceptron algorithms"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger",
        "pdf": "http://papers.nips.cc/paper/3300-bayesian-inference-for-spiking-neuron-models-with-a-sparsity-prior.pdf",
        "title": "Bayesian Inference for Spiking Neuron Models with a Sparsity Prior"
    },
    {
        "abstract": "We propose a method for reconstruction of human brain states directly from functional neuroimaging data. The method extends the traditional multivariate regression analysis of discretized fMRI data to the domain of stochastic functional measurements, facilitating evaluation of brain responses to naturalistic stimuli and boosting the power of functional imaging. The method searches for sets of voxel timecourses that optimize a multivariate functional linear model in terms of Rsquare-statistic. Population based incremental learning is used to search for spatially distributed voxel clusters, taking into account the variation in Haemodynamic lag across brain areas and among subjects by voxel-wise non-linear registration of stimuli to fMRI data. The method captures spatially distributed brain responses to naturalistic stimuli without attempting to localize function. Application of the method for prediction of naturalistic stimuli from new and unknown fMRI data shows that the approach is capable of identifying distributed clusters of brain locations that are highly predictive of a specific stimuli.",
        "authors": "Sennay Ghebreab, Arnold Smeulders, Pieter Adriaans",
        "pdf": "http://papers.nips.cc/paper/3326-predicting-brain-states-from-fmri-data-incremental-functional-principal-component-regression.pdf",
        "title": "Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression"
    },
    {
        "abstract": "We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-fire (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a self-regulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be flexibly configured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efficiently classify overlapping patterns, thanks to the self-regulating mechanism.",
        "authors": "Massimiliano Giulioni, Mario Pannunzi, Davide Badoni, Vittorio Dante, Paolo D. Giudice",
        "pdf": "http://papers.nips.cc/paper/3316-a-configurable-analog-vlsi-neural-network-with-spiking-neurons-and-self-regulating-plastic-synapses.pdf",
        "title": "A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses"
    },
    {
        "abstract": "We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.",
        "authors": "Amir Globerson, Tommi S. Jaakkola",
        "pdf": "http://papers.nips.cc/paper/3200-fixing-max-product-convergent-message-passing-algorithms-for-map-lp-relaxations.pdf",
        "title": "Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations"
    },
    {
        "abstract": "It is known that determinining whether a DEC-POMDP, namely, a cooperative partially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how cooperation affected that complexity. We show that, for competitive POSGs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class NEXP with an oracle for NP.",
        "authors": "Judy Goldsmith, Martin Mundhenk",
        "pdf": "http://papers.nips.cc/paper/3163-competition-adds-complexity.pdf",
        "title": "Competition Adds Complexity"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Kuzman Ganchev, Ben Taskar, João Gama",
        "pdf": "http://papers.nips.cc/paper/3170-expectation-maximization-and-posterior-constraints.pdf",
        "title": "Expectation Maximization and Posterior Constraints"
    },
    {
        "abstract": "On-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e. the movement of the pen, is recorded directly. However, the raw data can be difficult to interpret because each letter is spread over many pen locations. As a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as HMMs. In this paper we describe a system capable of directly transcribing raw on-line handwriting data. The system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark HMM in both cases.",
        "authors": "Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández",
        "pdf": "http://papers.nips.cc/paper/3213-unconstrained-on-line-handwriting-recognition-with-recurrent-neural-networks.pdf",
        "title": "Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Arthur Gretton, Kenji Fukumizu, Choon H. Teo, Le Song, Bernhard Schölkopf, Alex J. Smola",
        "pdf": "http://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf",
        "title": "A Kernel Statistical Test of Independence"
    },
    {
        "abstract": "Active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier. Most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration. However, single instance selection systems are unable to exploit a parallelized labeler when one is available. Recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration, guided by some heuristic scores. In this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables. The optimization is formuated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account. Although the objective is not convex, we can manipulate a quasi-Newton method to obtain a good local solution. Our empirical studies on UCI datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.",
        "authors": "Yuhong Guo, Dale Schuurmans",
        "pdf": "http://papers.nips.cc/paper/3295-discriminative-batch-mode-active-learning.pdf",
        "title": "Discriminative Batch Mode Active Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Yuhong Guo, Dale Schuurmans",
        "pdf": "http://papers.nips.cc/paper/3257-convex-relaxations-of-latent-variable-training.pdf",
        "title": "Convex Relaxations of Latent Variable Training"
    },
    {
        "abstract": "We propose to test for the homogeneity of two samples by using Kernel Fisher discriminant Analysis. This provides us with a consistent nonparametric test statistic, for which we derive the asymptotic distribution under the null hypothesis. We give experimental evidence of the relevance of our method on both artificial and real datasets.",
        "authors": "Moulines Eric, Francis R. Bach, Zaïd Harchaoui",
        "pdf": "http://papers.nips.cc/paper/3335-testing-for-homogeneity-with-kernel-fisher-discriminant-analysis.pdf",
        "title": "Testing for Homogeneity with Kernel Fisher Discriminant Analysis"
    },
    {
        "abstract": "We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-squares criterion with a l1-type penalty for this purpose. We prove that, in an appropriate asymptotic framework, this method provides consistent estimators of the change-points. Then, we explain how to implement this method in practice by combining the LAR algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data.",
        "authors": "Céline Levy-leduc, Zaïd Harchaoui",
        "pdf": "http://papers.nips.cc/paper/3188-catching-change-points-with-lasso.pdf",
        "title": "Catching Change-points with Lasso"
    },
    {
        "abstract": "We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modelled by players using no regret algorithms, which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithm's suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium.",
        "authors": "Elad Hazan, Satyen Kale",
        "pdf": "http://papers.nips.cc/paper/3249-computational-equivalence-of-fixed-points-and-no-regret-algorithms-and-convergence-to-equilibria.pdf",
        "title": "Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Jingrui He, Jaime G. Carbonell",
        "pdf": "http://papers.nips.cc/paper/3344-nearest-neighbor-based-active-learning-for-rare-category-detection.pdf",
        "title": "Nearest-Neighbor-Based Active Learning for Rare Category Detection"
    },
    {
        "abstract": "We propose a novel method for {\\em linear} dimensionality reduction of manifold modeled data. First, we show that with a small number M of {\\em random projections} of sample points in \\reals^N belonging to an unknown K-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number random projections required is linear in K and logarithmic in N, meaning that K<M\\ll N. To handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning. Our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition, storage and transmission costs.",
        "authors": "Chinmay Hegde, Michael Wakin, Richard Baraniuk",
        "pdf": "http://papers.nips.cc/paper/3191-random-projections-for-manifold-learning.pdf",
        "title": "Random Projections for Manifold Learning"
    },
    {
        "abstract": "We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efficient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with significant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).",
        "authors": "José M. Hernández-lobato, Tjeerd Dijkstra, Tom Heskes",
        "pdf": "http://papers.nips.cc/paper/3362-regulator-discovery-from-gene-expression-time-series-of-malaria-parasites-a-hierachical-approach.pdf",
        "title": "Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach"
    },
    {
        "abstract": "This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics. This ``eigenmodel'' generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.",
        "authors": "Peter Hoff",
        "pdf": "http://papers.nips.cc/paper/3294-modeling-homophily-and-stochastic-equivalence-in-symmetric-relational-data.pdf",
        "title": "Modeling homophily and stochastic equivalence in symmetric relational data"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Matthew Hoffman, Arnaud Doucet, Nando D. Freitas, Ajay Jasra",
        "pdf": "http://papers.nips.cc/paper/3343-bayesian-policy-learning-with-trans-dimensional-mcmc.pdf",
        "title": "Bayesian Policy Learning with Trans-Dimensional MCMC"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Charles L. Isbell, Michael P. Holmes, Alexander G. Gray",
        "pdf": "http://papers.nips.cc/paper/3166-ultrafast-monte-carlo-for-statistical-summations.pdf",
        "title": "Ultrafast Monte Carlo for Statistical Summations"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andrew Howard, Tony Jebara",
        "pdf": "http://papers.nips.cc/paper/3245-learning-monotonic-transformations-for-classification.pdf",
        "title": "Learning Monotonic Transformations for Classification"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Wee S. Lee, Nan Rong, David Hsu",
        "pdf": "http://papers.nips.cc/paper/3291-what-makes-some-pomdp-problems-easy-to-approximate.pdf",
        "title": "What makes some POMDP problems easy to approximate?"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Jonathan Huang, Carlos Guestrin, Leonidas J. Guibas",
        "pdf": "http://papers.nips.cc/paper/3183-efficient-inference-for-distributions-on-permutations.pdf",
        "title": "Efficient Inference for Distributions on Permutations"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Marcus Hutter, Shane Legg",
        "pdf": "http://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf",
        "title": "Temporal Difference Updating without a Learning Rate"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Tony Jebara, Yingbo Song, Kapil Thadani",
        "pdf": "http://papers.nips.cc/paper/3288-density-estimation-under-independent-similarly-distributed-sampling-assumptions.pdf",
        "title": "Density Estimation under Independent Similarly Distributed Sampling Assumptions"
    },
    {
        "abstract": "Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must find a good counter-strategy to the inferred posterior of the other agents' behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents' expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modified game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold'em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses.",
        "authors": "Michael Johanson, Martin Zinkevich, Michael Bowling",
        "pdf": "http://papers.nips.cc/paper/3347-computing-robust-counter-strategies.pdf",
        "title": "Computing Robust Counter-Strategies"
    },
    {
        "abstract": "We present a new local approximation algorithm for computing MAP and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise Markov random field (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when G excludes some finite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is \\Theta(n) (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs). Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme.",
        "authors": "Kyomin Jung, Devavrat Shah",
        "pdf": "http://papers.nips.cc/paper/3186-local-algorithms-for-approximate-inference-in-minor-excluded-graphs.pdf",
        "title": "Local Algorithms for Approximate Inference in Minor-Excluded Graphs"
    },
    {
        "abstract": "When we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually. This approach is called multi-task learning (MTL) and has been studied extensively. Existing approaches to MTL often treat all the tasks as \\emph{uniformly related to each other and the relatedness of the tasks is controlled globally. For this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have significantly different solutions. In this paper, we propose a novel MTL algorithm that can overcome these problems. Our method makes use of a task network, which describes the relation structure among tasks. This allows us to deal with intricate relation structures in a systematic way. Furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines (SVMs) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems.",
        "authors": "Tsuyoshi Kato, Hisashi Kashima, Masashi Sugiyama, Kiyoshi Asai",
        "pdf": "http://papers.nips.cc/paper/3368-multi-task-learning-via-conic-programming.pdf",
        "title": "Multi-Task Learning via Conic Programming"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Michael Kearns, Jinsong Tan, Jennifer Wortman",
        "pdf": "http://papers.nips.cc/paper/3302-privacy-preserving-belief-propagation-and-sampling.pdf",
        "title": "Privacy-Preserving Belief Propagation and Sampling"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Charles Kemp, Noah Goodman, Joshua B. Tenenbaum",
        "pdf": "http://papers.nips.cc/paper/3332-learning-and-using-relational-theories.pdf",
        "title": "Learning and using relational theories"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Sergey Kirshner",
        "pdf": "http://papers.nips.cc/paper/3185-learning-with-tree-averaged-densities-and-distributions.pdf",
        "title": "Learning with Tree-Averaged Densities and Distributions"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "J. Z. Kolter, Pieter Abbeel, Andrew Y. Ng",
        "pdf": "http://papers.nips.cc/paper/3253-hierarchical-apprenticeship-learning-with-application-to-quadruped-locomotion.pdf",
        "title": "Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andreas Krause, Brendan Mcmahan, Carlos Guestrin, Anupam Gupta",
        "pdf": "http://papers.nips.cc/paper/3341-selecting-observations-against-adversarial-objectives.pdf",
        "title": "Selecting Observations against Adversarial Objectives"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Alex Kulesza, Fernando Pereira",
        "pdf": "http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf",
        "title": "Structured Learning with Approximate Inference"
    },
    {
        "abstract": "We propose a randomized algorithm for large scale SVM learning which solves the problem by iterating over random subsets of the data. Crucial to the algorithm for scalability is the size of the subsets chosen. In the context of text classification we show that, by using ideas from random projections, a sample size of O(log n) can be used to obtain a solution which is close to the optimal with a high probability. Experiments done on synthetic and real life data sets demonstrate that the algorithm scales up SVM learners, without loss in accuracy.",
        "authors": "Krishnan Kumar, Chiru Bhattacharya, Ramesh Hariharan",
        "pdf": "http://papers.nips.cc/paper/3352-a-randomized-algorithm-for-large-scale-support-vector-learning.pdf",
        "title": "A Randomized Algorithm for Large Scale Support Vector Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Larry Wasserman, John D. Lafferty",
        "pdf": "http://papers.nips.cc/paper/3376-statistical-analysis-of-semi-supervised-regression.pdf",
        "title": "Statistical Analysis of Semi-Supervised Regression"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Yiu M. Lam, Bertram E. Shi",
        "pdf": "http://papers.nips.cc/paper/3221-extending-positionphase-shift-tuning-to-motion-energy-neurons-improves-velocity-discrimination.pdf",
        "title": "Extending position/phase-shift tuning to motion energy neurons improves velocity discrimination"
    },
    {
        "abstract": "We present Epoch-Greedy, an algorithm for multi-armed bandits with observable side information. Epoch-Greedy has the following properties: No knowledge of a time horizon T is necessary. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. The regret scales as O(T^{2/3} S^{1/3}) or better (sometimes, much better). Here S is the complexity term in a sample complexity bound for standard supervised learning.",
        "authors": "John Langford, Tong Zhang",
        "pdf": "http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information.pdf",
        "title": "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Danial Lashkari, Polina Golland",
        "pdf": "http://papers.nips.cc/paper/3181-convex-clustering-with-exemplar-based-models.pdf",
        "title": "Convex Clustering with Exemplar-Based Models"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Alessandro Lazaric, Marcello Restelli, Andrea Bonarini",
        "pdf": "http://papers.nips.cc/paper/3318-reinforcement-learning-in-continuous-action-spaces-through-sequential-monte-carlo-methods.pdf",
        "title": "Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods"
    },
    {
        "abstract": "We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited.",
        "authors": "Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl",
        "pdf": "http://papers.nips.cc/paper/3206-learning-the-2-d-topology-of-images.pdf",
        "title": "Learning the 2-D Topology of Images"
    },
    {
        "abstract": "Guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.",
        "authors": "Nicolas L. Roux, Pierre-antoine Manzagol, Yoshua Bengio",
        "pdf": "http://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm.pdf",
        "title": "Topmoumoute Online Natural Gradient Algorithm"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Guy Lebanon, Yi Mao",
        "pdf": "http://papers.nips.cc/paper/3175-non-parametric-modeling-of-partially-ranked-data.pdf",
        "title": "Non-parametric Modeling of Partially Ranked Data"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andrea Lecchini-visintini, John Lygeros, Jan Maciejowski",
        "pdf": "http://papers.nips.cc/paper/3298-simulated-annealing-rigorous-finite-time-guarantees-for-optimization-on-continuous-domains.pdf",
        "title": "Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains"
    },
    {
        "abstract": "Motivated in part by the hierarchical organization of cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or ``deep,'' structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both collinear (``contour'') features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex ``corner'' features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.",
        "authors": "Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng",
        "pdf": "http://papers.nips.cc/paper/3313-sparse-deep-belief-net-model-for-visual-area-v2.pdf",
        "title": "Sparse deep belief net model for visual area V2"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Dejan Pecevski, Wolfgang Maass, Robert A. Legenstein",
        "pdf": "http://papers.nips.cc/paper/3349-theoretical-analysis-of-learning-with-reward-modulated-spike-timing-dependent-plasticity.pdf",
        "title": "Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Máté Lengyel, Peter Dayan",
        "pdf": "http://papers.nips.cc/paper/3311-hippocampal-contributions-to-control-the-third-way.pdf",
        "title": "Hippocampal Contributions to Control: The Third Way"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ping Li, Qiang Wu, Christopher J. Burges",
        "pdf": "http://papers.nips.cc/paper/3270-mcrank-learning-to-rank-using-multiple-classification-and-gradient-boosting.pdf",
        "title": "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ping Li, Trevor J. Hastie",
        "pdf": "http://papers.nips.cc/paper/3225-a-unified-near-optimal-estimator-for-dimension-reduction-in-l_alpha-0alphaleq-2-using-stable-random-projections.pdf",
        "title": "A Unified Near-Optimal Estimator For Dimension Reduction in l_\\alpha (0<\\alpha\\leq 2) Using Stable Random Projections"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Percy S. Liang, Dan Klein, Michael I. Jordan",
        "pdf": "http://papers.nips.cc/paper/3246-agreement-based-learning.pdf",
        "title": "Agreement-Based Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Yuanqing Lin, Jingdong Chen, Youngmoo Kim, Daniel D. Lee",
        "pdf": "http://papers.nips.cc/paper/3262-blind-channel-identification-for-speech-dereverberation-using-l1-norm-sparse-learning.pdf",
        "title": "Blind channel identification for speech dereverberation using l1-norm sparse learning"
    },
    {
        "abstract": "Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we first develop an infrastructure for the automated crawling, parsing, and database storage of open source software. The infrastructure allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, SLOC, and method call distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the AUC metric to 0.86-- roughly 10-30% better than previous approaches based on text alone.",
        "authors": "Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, Pierre F. Baldi",
        "pdf": "http://papers.nips.cc/paper/3171-mining-internet-scale-software-repositories.pdf",
        "title": "Mining Internet-Scale Software Repositories"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Qiuhua Liu, Xuejun Liao, Lawrence Carin",
        "pdf": "http://papers.nips.cc/paper/3198-semi-supervised-multitask-learning.pdf",
        "title": "Semi-Supervised Multitask Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Phil Long, Rocco Servedio",
        "pdf": "http://papers.nips.cc/paper/3247-boosting-the-area-under-the-roc-curve.pdf",
        "title": "Boosting the Area under the ROC Curve"
    },
    {
        "abstract": "In this paper, we propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss. This can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method. We compare the performance of our technique with other methods on several data sets.",
        "authors": "Ronny Luss, Alexandre D'aspremont",
        "pdf": "http://papers.nips.cc/paper/3339-support-vector-machine-classification-with-indefinite-kernels.pdf",
        "title": "Support Vector Machine Classification with Indefinite Kernels"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ulrike V. Luxburg, Stefanie Jegelka, Michael Kaufmann, Sébastien Bubeck",
        "pdf": "http://papers.nips.cc/paper/3353-consistent-minimization-of-clustering-objective-functions.pdf",
        "title": "Consistent Minimization of Clustering Objective Functions"
    },
    {
        "abstract": "Stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity. Receptive fields are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons? Here, we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive fields span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive fields from multi-dimensional neural measurements such as those obtained from dynamic imaging methods.",
        "authors": "Guenther Zeck, Matthias Bethge, Jakob H. Macke",
        "pdf": "http://papers.nips.cc/paper/3220-receptive-fields-without-spike-triggering.pdf",
        "title": "Receptive Fields without Spike-Triggering"
    },
    {
        "abstract": "We present a new and efficient semi-supervised training method for parameter estimation and feature selection in conditional random fields (CRFs). In real-world applications such as activity recognition, unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect. Furthermore, the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy. In this paper, we introduce the semi-supervised virtual evidence boosting (sVEB) algorithm for training CRFs -- a semi-supervised extension to the recently developed virtual evidence boosting (VEB) method for feature selection and parameter learning. Semi-supervised VEB takes advantage of the unlabeled data via minimum entropy regularization -- the objective function combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood. The sVEB algorithm reduces the overall system cost as well as the human labeling cost required during training, which are both important considerations in building real world inference systems. In a set of experiments on synthetic data and real activity traces collected from wearable sensors, we illustrate that our algorithm benefits from both the use of unlabeled data and automatic feature selection, and outperforms other semi-supervised training approaches.",
        "authors": "Maryam Mahdaviani, Tanzeem Choudhury",
        "pdf": "http://papers.nips.cc/paper/3348-fast-and-scalable-training-of-semi-supervised-crfs-with-application-to-activity-recognition.pdf",
        "title": "Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "M. M. Mahmud, Sylvian Ray",
        "pdf": "http://papers.nips.cc/paper/3228-transfer-learning-using-kolmogorov-complexity-basic-theory-and-empirical-evaluations.pdf",
        "title": "Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations"
    },
    {
        "abstract": "We address the problem of adaptive sensor control in dynamic resource-constrained sensor networks. We focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360 degrees. We compare three sector scanning strategies. The sit-and-spin strategy always scans 360 degrees. The limited lookahead strategy additionally uses the expected environmental state K decision epochs in the future, as predicted from Kalman filters, in its decision-making. The full lookahead strategy uses all expected future states by casting the problem as a Markov decision process and using reinforcement learning to estimate the optimal scan strategy. We show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars. We also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned.",
        "authors": "Victoria Manfredi, Jim Kurose",
        "pdf": "http://papers.nips.cc/paper/3199-scan-strategies-for-meteorological-radars.pdf",
        "title": "Scan Strategies for Meteorological Radars"
    },
    {
        "abstract": "Functional Magnetic Resonance Imaging (fMRI) provides an unprecedented window into the complex functioning of the human brain, typically detailing the activity of thousands of voxels during hundreds of sequential time points. Unfortunately, the interpretation of fMRI is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristics of the cognitive patterns themselves. Here, we use data from the Experience Based Cognition competition to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction. We build global low dimensional representations of an fMRI dataset, using linear and nonlinear methods. We learn a set of time series that are implicit functions of the fMRI data, and predict the values of these times series in the future from the knowledge of the fMRI data only. We find effective, low-dimensional models based on the principal components of cognitive activity in classically-defined anatomical regions, the Brodmann Areas. Furthermore for some of the stimuli, the top predictive regions were stable across subjects and episodes, including WernickeÕs area for verbal instructions, visual cortex for facial and body features, and visual-temporal regions (Brodmann Area 7) for velocity. These interpretations and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated techniques for fMRI decoding. To our knowledge, this is the first time that classical areas have been used in fMRI for an effective prediction of complex natural experience.",
        "authors": "Francois Meyer, Greg Stephens",
        "pdf": "http://papers.nips.cc/paper/3320-locality-and-low-dimensions-in-the-prediction-of-natural-experience-from-fmri.pdf",
        "title": "Locality and low-dimensions in the prediction of natural experience from fMRI"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Srinjoy Mitra, Giacomo Indiveri, Stefano Fusi",
        "pdf": "http://papers.nips.cc/paper/3161-learning-to-classify-complex-patterns-using-a-vlsi-network-of-spiking-neurons.pdf",
        "title": "Learning to classify complex patterns using a VLSI network of spiking neurons"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Daichi Mochihashi, Eiichiro Sumita",
        "pdf": "http://papers.nips.cc/paper/3281-the-infinite-markov-model.pdf",
        "title": "The Infinite Markov Model"
    },
    {
        "abstract": "The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are de- signed for specific learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. We also illustrate their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.",
        "authors": "Mehryar Mohri, Afshin Rostamizadeh",
        "pdf": "http://papers.nips.cc/paper/3239-stability-bounds-for-non-iid-processes.pdf",
        "title": "Stability Bounds for Non-i.i.d. Processes"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "David Baldwin, Michael C. Mozer",
        "pdf": "http://papers.nips.cc/paper/3301-experience-guided-search-a-theory-of-attentional-control.pdf",
        "title": "Experience-Guided Search: A Theory of Attentional Control"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Pawan Mudigonda, Vladimir Kolmogorov, Philip Torr",
        "pdf": "http://papers.nips.cc/paper/3373-an-analysis-of-convex-relaxations-for-map-estimation.pdf",
        "title": "An Analysis of Convex Relaxations for MAP Estimation"
    },
    {
        "abstract": "We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.",
        "authors": "Lawrence Murray, Amos J. Storkey",
        "pdf": "http://papers.nips.cc/paper/3172-continuous-time-particle-filtering-for-fmri.pdf",
        "title": "Continuous Time Particle Filtering for fMRI"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andrew Naish-guzman, Sean Holden",
        "pdf": "http://papers.nips.cc/paper/3351-the-generalized-fitc-approximation.pdf",
        "title": "The Generalized FITC Approximation"
    },
    {
        "abstract": "We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp (2000) and Rasmussen and Ghahramani (2002). The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. (1998), and the more recent contributions of Tresp, and Rasmussen and Ghahramani.",
        "authors": "Andrew Naish-guzman, Sean Holden",
        "pdf": "http://papers.nips.cc/paper/3346-robust-regression-with-twinned-gaussian-processes.pdf",
        "title": "Robust Regression with Twinned Gaussian Processes"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Emre Neftci, Elisabetta Chicca, Giacomo Indiveri, Jean-jeacques Slotine, Rodney J. Douglas",
        "pdf": "http://papers.nips.cc/paper/3364-contraction-properties-of-vlsi-cooperative-competitive-neural-networks-of-spiking-neurons.pdf",
        "title": "Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "David Newman, Padhraic Smyth, Max Welling, Arthur U. Asuncion",
        "pdf": "http://papers.nips.cc/paper/3330-distributed-inference-for-latent-dirichlet-allocation.pdf",
        "title": "Distributed Inference for Latent Dirichlet Allocation"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "XuanLong Nguyen, Martin J. Wainwright, Michael I. Jordan",
        "pdf": "http://papers.nips.cc/paper/3193-estimating-divergence-functionals-and-the-likelihood-ratio-by-penalized-convex-risk-minimization.pdf",
        "title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Shigeyuki Oba, Motoaki Kawanabe, Klaus-Robert Müller, Shin Ishii",
        "pdf": "http://papers.nips.cc/paper/3222-heterogeneous-component-analysis.pdf",
        "title": "Heterogeneous Component Analysis"
    },
    {
        "abstract": "Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean field approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, {while still retaining a good degree of accuracy.} We illustrate our approach on two biologically motivated systems.",
        "authors": "Manfred Opper, Guido Sanguinetti",
        "pdf": "http://papers.nips.cc/paper/3296-variational-inference-for-markov-jump-processes.pdf",
        "title": "Variational inference for Markov jump processes"
    },
    {
        "abstract": "This paper proposes constraint propagation relaxation (CPR), a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms SP(&#961;), ranging from belief propagation (&#961; = 0) to (pure) survey propagation(&#961; = 1). More importantly, the approach elucidates the implicit, but fundamental assumptions underlying SP(&#961;), thus shedding some light on its effectiveness and leading to applications beyond k-SAT.",
        "authors": "Luis E. Ortiz",
        "pdf": "http://papers.nips.cc/paper/3361-cpr-for-csps-a-probabilistic-relaxation-of-constraint-propagation.pdf",
        "title": "CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation"
    },
    {
        "abstract": "We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.",
        "authors": "Simon Osindero, Geoffrey E. Hinton",
        "pdf": "http://papers.nips.cc/paper/3279-modeling-image-patches-with-a-directed-hierarchy-of-markov-random-fields.pdf",
        "title": "Modeling image patches with a directed hierarchy of Markov random fields"
    },
    {
        "abstract": "This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.",
        "authors": "Mehul Parsana, Sourangshu Bhattacharya, Chiru Bhattacharya, K. Ramakrishnan",
        "pdf": "http://papers.nips.cc/paper/3304-kernels-on-attributed-pointsets-with-applications.pdf",
        "title": "Kernels on Attributed Pointsets with Applications"
    },
    {
        "abstract": "This paper explores the use of a Maximal Average Margin (MAM) optimality principle for the design of learning algorithms. It is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical Parzen window classifier. A direct relation with the Rademacher complexities is established, as such facilitating analysis and providing a notion of certainty of prediction. This analysis is related to Support Vector Machines by means of a margin transformation. The power of the MAM principle is illustrated further by application to ordinal regression tasks, resulting in an O(n) algorithm able to process large datasets in reasonable time.",
        "authors": "Kristiaan Pelckmans, Johan Suykens, Bart D. Moor",
        "pdf": "http://papers.nips.cc/paper/3243-a-risk-minimization-principle-for-a-class-of-parzen-estimators.pdf",
        "title": "A Risk Minimization Principle for a Class of Parzen Estimators"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Robert Peters, Laurent Itti",
        "pdf": "http://papers.nips.cc/paper/3360-congruence-between-model-and-human-attention-reveals-unique-signatures-of-critical-visual-events.pdf",
        "title": "Congruence between model and human attention reveals unique signatures of critical visual events"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Slav Petrov, Dan Klein",
        "pdf": "http://papers.nips.cc/paper/3337-discriminative-log-linear-grammars-with-latent-variables.pdf",
        "title": "Discriminative Log-Linear Grammars with Latent Variables"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Jonathan W. Pillow, Peter E. Latham",
        "pdf": "http://papers.nips.cc/paper/3256-neural-characterization-in-partially-observed-populations-of-spiking-neurons.pdf",
        "title": "Neural characterization in partially observed populations of spiking neurons"
    },
    {
        "abstract": "Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 10^4 possible faults from 10^5 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a variational approximation, a mean-field approximation, and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours.",
        "authors": "Emre Kiciman, David Maltz, John C. Platt",
        "pdf": "http://papers.nips.cc/paper/3292-fast-variational-inference-for-large-scale-internet-diagnosis.pdf",
        "title": "Fast Variational Inference for Large-scale Internet Diagnosis"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ali Rahimi, Benjamin Recht",
        "pdf": "http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf",
        "title": "Random Features for Large-Scale Kernel Machines"
    },
    {
        "abstract": "Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machines trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input variables can be captured.",
        "authors": "Marc'aurelio Ranzato, Y-lan Boureau, Yann L. Cun",
        "pdf": "http://papers.nips.cc/paper/3363-sparse-feature-learning-for-deep-belief-networks.pdf",
        "title": "Sparse Feature Learning for Deep Belief Networks"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Vinayak Rao, Marc Howard",
        "pdf": "http://papers.nips.cc/paper/3232-retrieved-context-and-the-discovery-of-semantic-structure.pdf",
        "title": "Retrieved context and the discovery of semantic structure"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Han Liu, Larry Wasserman, John D. Lafferty, Pradeep K. Ravikumar",
        "pdf": "http://papers.nips.cc/paper/3194-spam-sparse-additive-models.pdf",
        "title": "SpAM: Sparse Additive Models"
    },
    {
        "abstract": "In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantifies the quality of rankings, is the standard performance measure for model \\emph{assessment} in survival analysis. In contrast, the standard approach to \\emph{learning} the popular proportional hazard (PH) model is based on Cox's partial likelihood. In this paper we devise two bounds on CI--one of which emerges directly from the properties of PH models--and optimize them \\emph{directly}. Our experimental results suggest that both methods perform about equally well, with our new approach giving slightly better results than the Cox's method. We also explain why a method designed to maximize the Cox's partial likelihood also ends up (approximately) maximizing the CI.",
        "authors": "Harald Steck, Balaji Krishnapuram, Cary Dehing-oberije, Philippe Lambin, Vikas C. Raykar",
        "pdf": "http://papers.nips.cc/paper/3375-on-ranking-in-survival-analysis-bounds-on-the-concordance-index.pdf",
        "title": "On Ranking in Survival Analysis: Bounds on the Concordance Index"
    },
    {
        "abstract": "This paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects' performance on different visual classification tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classifier operating on raw image pixels. Instead, it models classification as the combination of multiple feature detectors. This approach extracts more information about human visual classification than has been previously possible with other methods and provides a foundation for further exploration.",
        "authors": "Michael Ross, Andrew Cohen",
        "pdf": "http://papers.nips.cc/paper/3289-grift-a-graphical-model-for-inferring-visual-classification-features-from-human-data.pdf",
        "title": "GRIFT: A graphical model for inferring visual classification features from human data"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Stephane Ross, Brahim Chaib-draa, Joelle Pineau",
        "pdf": "http://papers.nips.cc/paper/3333-bayes-adaptive-pomdps.pdf",
        "title": "Bayes-Adaptive POMDPs"
    },
    {
        "abstract": "Planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts. Thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and epsilon-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.",
        "authors": "Stephane Ross, Joelle Pineau, Brahim Chaib-draa",
        "pdf": "http://papers.nips.cc/paper/3250-theoretical-analysis-of-heuristic-search-methods-for-online-pomdps.pdf",
        "title": "Theoretical Analysis of Heuristic Search Methods for Online POMDPs"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Bryan Russell, Antonio Torralba, Ce Liu, Rob Fergus, William T. Freeman",
        "pdf": "http://papers.nips.cc/paper/3254-object-recognition-by-scene-alignment.pdf",
        "title": "Object Recognition by Scene Alignment"
    },
    {
        "abstract": "We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by Hinton et.al. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.",
        "authors": "Geoffrey E. Hinton, Russ R. Salakhutdinov",
        "pdf": "http://papers.nips.cc/paper/3211-using-deep-belief-nets-to-learn-covariance-kernels-for-gaussian-processes.pdf",
        "title": "Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Andriy Mnih, Russ R. Salakhutdinov",
        "pdf": "http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf",
        "title": "Probabilistic Matrix Factorization"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Adam Sanborn, Thomas L. Griffiths",
        "pdf": "http://papers.nips.cc/paper/3214-markov-chain-monte-carlo-with-people.pdf",
        "title": "Markov Chain Monte Carlo with People"
    },
    {
        "abstract": "Loopy belief propagation has been employed in a wide variety of applications with great empirical success, but it comes with few theoretical guarantees. In this paper we investigate the use of the max-product form of belief propagation for weighted matching problems on general graphs. We show that max-product converges to the correct answer if the linear programming (LP) relaxation of the weighted matching problem is tight and does not converge if the LP relaxation is loose. This provides an exact characterization of max-product performance and reveals connections to the widely used optimization technique of LP relaxation. In addition, we demonstrate that max-product is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of self-organization in sensor networks.",
        "authors": "Sujay Sanghavi, Dmitry Malioutov, Alan S. Willsky",
        "pdf": "http://papers.nips.cc/paper/3285-linear-programming-analysis-of-loopy-belief-propagation-for-weighted-matching.pdf",
        "title": "Linear programming analysis of loopy belief propagation for weighted matching"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Sujay Sanghavi, Devavrat Shah, Alan S. Willsky",
        "pdf": "http://papers.nips.cc/paper/3240-message-passing-for-max-weight-independent-set.pdf",
        "title": "Message Passing for Max-weight Independent Set"
    },
    {
        "abstract": "In a multiple instance (MI) learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We present a framework for active learning in the multiple-instance setting. In particular, we consider the case in which an MI learner is allowed to selectively query unlabeled instances in positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can significantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image recognition and text classification.",
        "authors": "Burr Settles, Mark Craven, Soumya Ray",
        "pdf": "http://papers.nips.cc/paper/3252-multiple-instance-active-learning.pdf",
        "title": "Multiple-Instance Active Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ohad Shamir, Naftali Tishby",
        "pdf": "http://papers.nips.cc/paper/3227-cluster-stability-for-finite-samples.pdf",
        "title": "Cluster Stability for Finite Samples"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Tatyana Sharpee",
        "pdf": "http://papers.nips.cc/paper/3242-better-than-least-squares-comparison-of-objective-functions-for-estimating-linear-nonlinear-models.pdf",
        "title": "Better than least squares: comparison of objective functions for estimating linear-nonlinear models"
    },
    {
        "abstract": "An important problem in many fields is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and also do not have a provision to control the expressiveness\" of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.\"",
        "authors": "Madhusudana Shashanka, Bhiksha Raj, Paris Smaragdis",
        "pdf": "http://papers.nips.cc/paper/3235-sparse-overcomplete-latent-variable-decomposition-of-counts-data.pdf",
        "title": "Sparse Overcomplete Latent Variable Decomposition of Counts Data"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "M.a. S. Elmohamed, Dexter Kozen, Daniel R. Sheldon",
        "pdf": "http://papers.nips.cc/paper/3315-collective-inference-on-markov-models-for-modeling-bird-migration.pdf",
        "title": "Collective Inference on Markov Models for Modeling Bird Migration"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Byron Boots, Geoffrey J. Gordon, Sajid M. Siddiqi",
        "pdf": "http://papers.nips.cc/paper/3358-a-constraint-generation-approach-to-learning-stable-linear-dynamical-systems.pdf",
        "title": "A Constraint Generation Approach to Learning Stable Linear Dynamical Systems"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Leonid Sigal, Alexandru Balan, Michael J. Black",
        "pdf": "http://papers.nips.cc/paper/3271-combined-discriminative-and-generative-articulated-pose-and-non-rigid-shape-estimation.pdf",
        "title": "Combined discriminative and generative articulated pose and non-rigid shape estimation"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ricardo Silva, Wei Chu, Zoubin Ghahramani",
        "pdf": "http://papers.nips.cc/paper/3276-hidden-common-cause-relations-in-relational-learning.pdf",
        "title": "Hidden Common Cause Relations in Relational Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Vikas Singh, Lopamudra Mukherjee, Jiming Peng, Jinhui Xu",
        "pdf": "http://papers.nips.cc/paper/3283-ensemble-clustering-using-semidefinite-programming.pdf",
        "title": "Ensemble Clustering using Semidefinite Programming"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Kaushik Sinha, Mikhail Belkin",
        "pdf": "http://papers.nips.cc/paper/3345-the-value-of-labeled-and-unlabeled-examples-when-the-model-is-imperfect.pdf",
        "title": "The Value of Labeled and Unlabeled Examples when the Model is Imperfect"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Olivier Chapelle, Alekh Agarwal, Fabian H. Sinz, Bernhard Schölkopf",
        "pdf": "http://papers.nips.cc/paper/3231-an-analysis-of-inference-with-the-universum.pdf",
        "title": "An Analysis of Inference with the Universum"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Quoc V. Le, Alex J. Smola, S.v.n. Vishwanathan",
        "pdf": "http://papers.nips.cc/paper/3230-bundle-methods-for-machine-learning.pdf",
        "title": "Bundle Methods for Machine Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Le Song, Arthur Gretton, Karsten Borgwardt, Alex J. Smola",
        "pdf": "http://papers.nips.cc/paper/3307-colored-maximum-variance-unfolding.pdf",
        "title": "Colored Maximum Variance Unfolding"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "David Sontag, Tommi S. Jaakkola",
        "pdf": "http://papers.nips.cc/paper/3274-new-outer-bounds-on-the-marginal-polytope.pdf",
        "title": "New Outer Bounds on the Marginal Polytope"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Devarajan Sridharan, Brian Percival, John Arthur, Kwabena A. Boahen",
        "pdf": "http://papers.nips.cc/paper/3273-an-in-silico-neural-model-of-dynamic-routing-through-neuronal-coherence.pdf",
        "title": "An in-silico Neural Model of Dynamic Routing through Neuronal Coherence"
    },
    {
        "abstract": "We propose an extended probabilistic model for human perception. We argue that in many circumstances, human observers simultaneously evaluate sensory evidence under different hypotheses regarding the underlying physical process that might have generated the sensory information. Within this context, inference can be optimal if the observer weighs each hypothesis according to the correct belief in that hypothesis. But if the observer commits to a particular hypothesis, the belief in that hypothesis is converted into subjective certainty, and subsequent perceptual behavior is suboptimal, conditioned only on the chosen hypothesis. We demonstrate that this framework can explain psychophysical data of a recently reported decision-estimation experiment. The model well accounts for the data, predicting the same estimation bias as a consequence of the preceding decision step. The power of the framework is that it has no free parameters except the degree of the observer's uncertainty about its internal sensory representation. All other parameters are defined by the particular experiment which allows us to make quantitative predictions of human perception to two modifications of the original experiment.",
        "authors": "Alan A. Stocker, Eero P. Simoncelli",
        "pdf": "http://papers.nips.cc/paper/3369-a-bayesian-model-of-conditioned-perception.pdf",
        "title": "A Bayesian Model of Conditioned Perception"
    },
    {
        "abstract": "We provide a provably efficient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Specifically, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh's work that provides a provably efficient algorithm for finite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.",
        "authors": "Alexander L. Strehl, Michael L. Littman",
        "pdf": "http://papers.nips.cc/paper/3197-online-linear-regression-and-its-application-to-model-based-reinforcement-learning.pdf",
        "title": "Online Linear Regression and Its Application to Model-Based Reinforcement Learning"
    },
    {
        "abstract": "Variational methods are frequently used to approximate or bound the partition or likelihood function of a Markov random field. Methods based on mean field theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds. In general, loopy belief propagation (BP) provides (often accurate) approximations, but not bounds. We prove that for a class of attractive binary models, the value specified by any fixed point of loopy BP always provides a lower bound on the true likelihood. Empirically, this bound is much better than the naive mean field bound, and requires no further work than running BP. We establish these lower bounds using a loop series expansion due to Chertkov and Chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of BP fixed points.",
        "authors": "Alan S. Willsky, Erik B. Sudderth, Martin J. Wainwright",
        "pdf": "http://papers.nips.cc/paper/3354-loop-series-and-bethe-variational-bounds-in-attractive-graphical-models.pdf",
        "title": "Loop Series and Bethe Variational Bounds in Attractive Graphical Models"
    },
    {
        "abstract": "When training and test samples follow different input distributions (i.e., the situation called \\emph{covariate shift}), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the \\emph{importance} (i.e., the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.",
        "authors": "Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul V. Buenau, Motoaki Kawanabe",
        "pdf": "http://papers.nips.cc/paper/3248-direct-importance-estimation-with-model-selection-and-its-application-to-covariate-shift-adaptation.pdf",
        "title": "Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Ozgur Sumer, Umut Acar, Alexander T. Ihler, Ramgopal R. Mettu",
        "pdf": "http://papers.nips.cc/paper/3255-efficient-bayesian-inference-for-dynamically-changing-graphs.pdf",
        "title": "Efficient Bayesian Inference for Dynamically Changing Graphs"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Umar Syed, Robert E. Schapire",
        "pdf": "http://papers.nips.cc/paper/3293-a-game-theoretic-approach-to-apprenticeship-learning.pdf",
        "title": "A Game-Theoretic Approach to Apprenticeship Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Marie Szafranski, Yves Grandvalet, Pierre Morizet-mahoudeaux",
        "pdf": "http://papers.nips.cc/paper/3338-hierarchical-penalization.pdf",
        "title": "Hierarchical Penalization"
    },
    {
        "abstract": "The control of high-dimensional, continuous, non-linear systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP) are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper, we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing effectively with problems with (at least) 34 state and 14 action dimensions.",
        "authors": "Yuval Tassa, Tom Erez, William D. Smart",
        "pdf": "http://papers.nips.cc/paper/3297-receding-horizon-differential-dynamic-programming.pdf",
        "title": "Receding Horizon Differential Dynamic Programming"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Yee W. Teh, Hal Daume III, Daniel M. Roy",
        "pdf": "http://papers.nips.cc/paper/3266-bayesian-agglomerative-clustering-with-coalescents.pdf",
        "title": "Bayesian Agglomerative Clustering with Coalescents"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Yee W. Teh, Kenichi Kurihara, Max Welling",
        "pdf": "http://papers.nips.cc/paper/3342-collapsed-variational-inference-for-hdp.pdf",
        "title": "Collapsed Variational Inference for HDP"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Choon H. Teo, Amir Globerson, Sam T. Roweis, Alex J. Smola",
        "pdf": "http://papers.nips.cc/paper/3218-convex-learning-with-invariances.pdf",
        "title": "Convex Learning with Invariances"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart, David Levine, Freeman Rawson, Charles Lefurgy",
        "pdf": "http://papers.nips.cc/paper/3251-managing-power-consumption-and-performance-of-computing-systems-using-reinforcement-learning.pdf",
        "title": "Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning"
    },
    {
        "abstract": "We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates: a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time T is within C(P)\\log T of the reward obtained by the optimal policy, where C(P) is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in flavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP.",
        "authors": "Ambuj Tewari, Peter L. Bartlett",
        "pdf": "http://papers.nips.cc/paper/3329-optimistic-linear-programming-gives-logarithmic-regret-for-irreducible-mdps.pdf",
        "title": "Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs"
    },
    {
        "abstract": "We address the problem of factorial learning which associates a set of latent causes or features with the observed data. Factorial models usually assume that each feature has a single occurrence in a given data point. However, there are data such as images where latent features have multiple occurrences, e.g. a visual object class can have multiple instances shown in the same image. To deal with such cases, we present a probability model over non-negative integer valued matrices with possibly unbounded number of columns. This model can play the role of the prior in an nonparametric Bayesian learning scenario where both the latent features and the number of their occurrences are unknown. We use this prior together with a likelihood model for unsupervised learning from images using a Markov Chain Monte Carlo inference algorithm.",
        "authors": "Michalis K. Titsias",
        "pdf": "http://papers.nips.cc/paper/3309-the-infinite-gamma-poisson-feature-model.pdf",
        "title": "The Infinite Gamma-Poisson Feature Model"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Kristina Toutanova, Mark Johnson",
        "pdf": "http://papers.nips.cc/paper/3317-a-bayesian-lda-based-model-for-semi-supervised-part-of-speech-tagging.pdf",
        "title": "A Bayesian LDA-based model for semi-supervised part-of-speech tagging"
    },
    {
        "abstract": "Fair discriminative pedestrian finders are now available. In fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human configuration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that configuration to an SVM classifier. We show, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder.",
        "authors": "Duan Tran, David A. Forsyth",
        "pdf": "http://papers.nips.cc/paper/3210-configuration-estimates-improve-pedestrian-finding.pdf",
        "title": "Configuration Estimates Improve Pedestrian Finding"
    },
    {
        "abstract": "Binocular fusion takes place over a limited region smaller than one degree of visual angle (Panum's fusional area), which is on the order of the range of preferred disparities measured in populations of disparity-tuned neurons in the visual cortex. However, the actual range of binocular disparities encountered in natural scenes ranges over tens of degrees. This discrepancy suggests that there must be a mechanism for detecting whether the stimulus disparity is either inside or outside of the range of the preferred disparities in the population. Here, we present a statistical framework to derive feature in a population of V1 disparity neuron to determine the stimulus disparity within the preferred disparity range of the neural population. When optimized for natural images, it yields a feature that can be explained by the normalization which is a common model in V1 neurons. We further makes use of the feature to estimate the disparity in natural images. Our proposed model generates more correct estimates than coarse-to-fine multiple scales approaches and it can also identify regions with occlusion. The approach suggests another critical role for normalization in robust disparity estimation.",
        "authors": "Eric K. Tsang, Bertram E. Shi",
        "pdf": "http://papers.nips.cc/paper/3334-estimating-disparity-with-confidence-from-energy-neurons.pdf",
        "title": "Estimating disparity with confidence from energy neurons"
    },
    {
        "abstract": "Natural sounds are structured on many time-scales. A typical segment of speech, for example, contains features that span four orders of magnitude: Sentences (~1s); phonemes (~0.1s); glottal pulses (~0.01s); and formants (<0.001s). The auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis. One route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing. There is however a discord: Current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored. The reason for this is two-fold. Firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information. Secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information). The contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms. We demonstrate the success of this approach on a missing data task.",
        "authors": "Richard Turner, Maneesh Sahani",
        "pdf": "http://papers.nips.cc/paper/3366-modeling-natural-sounds-with-modulation-cascade-processes.pdf",
        "title": "Modeling Natural Sounds with Modulation Cascade Processes"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Bill Triggs, Jakob J. Verbeek",
        "pdf": "http://papers.nips.cc/paper/3268-scene-segmentation-with-crfs-learned-from-partially-labeled-images.pdf",
        "title": "Scene Segmentation with CRFs Learned from Partially Labeled Images"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Christian Walder, Olivier Chapelle",
        "pdf": "http://papers.nips.cc/paper/3215-learning-with-transformation-invariant-kernels.pdf",
        "title": "Learning with Transformation Invariant Kernels"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Tao Wang, Michael Bowling, Dale Schuurmans, Daniel J. Lizotte",
        "pdf": "http://papers.nips.cc/paper/3179-stable-dual-dynamic-programming.pdf",
        "title": "Stable Dual Dynamic Programming"
    },
    {
        "abstract": "In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely appled in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a ``bag-of-words''. It is also critical to properly design ``words'' and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structure among visual words that are essential for solving many vision problems. The spatial information is not encoded in the value of visual words but in the design of documents. Instead of knowing the partition of words into documents \\textit{a priori}, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA.",
        "authors": "Xiaogang Wang, Eric Grimson",
        "pdf": "http://papers.nips.cc/paper/3278-spatial-latent-dirichlet-allocation.pdf",
        "title": "Spatial Latent Dirichlet Allocation"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Gunnar Rätsch, Manfred K. K. Warmuth, Karen A. Glocer",
        "pdf": "http://papers.nips.cc/paper/3374-boosting-algorithms-for-maximizing-the-soft-margin.pdf",
        "title": "Boosting Algorithms for Maximizing the Soft Margin"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Markus Weimer, Alexandros Karatzoglou, Quoc V. Le, Alex J. Smola",
        "pdf": "http://papers.nips.cc/paper/3359-cofi-rank-maximum-margin-matrix-factorization-for-collaborative-ranking.pdf",
        "title": "COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Max Welling, Ian Porteous, Evgeniy Bart",
        "pdf": "http://papers.nips.cc/paper/3310-infinite-state-bayes-nets-for-structured-domains.pdf",
        "title": "Infinite State Bayes-Nets for Structured Domains"
    },
    {
        "abstract": "Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.",
        "authors": "Ben Williams, Marc Toussaint, Amos J. Storkey",
        "pdf": "http://papers.nips.cc/paper/3204-modelling-motion-primitives-and-their-timing-in-biologically-executed-movements.pdf",
        "title": "Modelling motion primitives and their timing in biologically executed movements"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "David Wingate, Satinder S. Baveja",
        "pdf": "http://papers.nips.cc/paper/3177-exponential-family-predictive-representations-of-state.pdf",
        "title": "Exponential Family Predictive Representations of State"
    },
    {
        "abstract": "Automatic relevance determination (ARD), and the closely-related sparse Bayesian learning (SBL) framework, are effective tools for pruning large numbers of irrelevant features. However, popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties. This paper furnishes an alternative means of optimizing a general ARD cost function using an auxiliary function that can naturally be solved using a series of re-weighted L1 problems. The result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods. The analysis also leads to additional insights into the behavior of previous ARD updates as well as the ARD cost function. For example, the standard fixed-point updates of MacKay (1992) are shown to be iteratively solving a particular min-max problem, although they are not guaranteed to lead to a stationary point. The analysis also reveals that ARD is exactly equivalent to performing MAP estimation using a particular feature- and noise-dependent \\textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection. In particular, it provides a tighter approximation to the L0 quasi-norm sparsity measure than the L1 norm. Overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.",
        "authors": "David P. Wipf, Srikantan S. Nagarajan",
        "pdf": "http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf",
        "title": "A New View of Automatic Relevance Determination"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, Heung-yeung Shum",
        "pdf": "http://papers.nips.cc/paper/3314-classification-via-minimum-incremental-coding-length-micl.pdf",
        "title": "Classification via Minimum Incremental Coding Length (MICL)"
    },
    {
        "abstract": "We consider the problem of Support Vector Machine transduction, which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples. Although several studies are devoted to Transductive SVM, they suffer either from the high computation complexity or from the solutions of local optimum. To address this problem, we propose solving Transductive SVM via a convex relaxation, which converts the NP-hard problem to a semi-definite programming. Compared with the other SDP relaxation for Transductive SVM, the proposed algorithm is computationally more efficient with the number of free parameters reduced from O(n2) to O(n) where n is the number of examples. Empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of Transductive SVM.",
        "authors": "Zenglin Xu, Rong Jin, Jianke Zhu, Irwin King, Michael Lyu",
        "pdf": "http://papers.nips.cc/paper/3356-efficient-convex-relaxation-for-transductive-support-vector-machine.pdf",
        "title": "Efficient Convex Relaxation for Transductive Support Vector Machine"
    },
    {
        "abstract": "We present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship, we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering, as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.",
        "authors": "Jieping Ye, Zheng Zhao, Mingrui Wu",
        "pdf": "http://papers.nips.cc/paper/3176-discriminative-k-means-for-clustering.pdf",
        "title": "Discriminative K-means for Clustering"
    },
    {
        "abstract": "In this paper we develop a Gaussian process (GP) framework to model a collection of reciprocal random variables defined on the \\emph{edges} of a network. We show how to construct GP priors, i.e.,~covariance functions, on the edges of directed, undirected, and bipartite graphs. The model suggests an intimate connection between \\emph{link prediction} and \\emph{transfer learning}, which were traditionally considered two separate research topics. Though a straightforward GP inference has a very high complexity, we develop an efficient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity.",
        "authors": "Kai Yu, Wei Chu",
        "pdf": "http://papers.nips.cc/paper/3284-gaussian-process-models-for-link-analysis-and-transfer-learning.pdf",
        "title": "Gaussian Process Models for Link Analysis and Transfer Learning"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Shipeng Yu, Balaji Krishnapuram, Harald Steck, R. B. Rao, Rómer Rosales",
        "pdf": "http://papers.nips.cc/paper/3260-bayesian-co-training.pdf",
        "title": "Bayesian Co-Training"
    },
    {
        "abstract": "We describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables. The distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution. We prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used. We illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts. Finally, we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.",
        "authors": "Hongjing Lu, Alan L. Yuille",
        "pdf": "http://papers.nips.cc/paper/3259-the-noisy-logical-distribution-and-its-application-to-causal-inference.pdf",
        "title": "The Noisy-Logical Distribution and its Application to Causal Inference"
    },
    {
        "abstract": "Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade earning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classifier can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate significant performance advantages.",
        "authors": "Cha Zhang, Paul A. Viola",
        "pdf": "http://papers.nips.cc/paper/3265-multiple-instance-pruning-for-learning-efficient-cascade-detectors.pdf",
        "title": "Multiple-Instance Pruning For Learning Efficient Cascade Detectors"
    },
    {
        "abstract": "We present a novel paradigm for statistical machine translation (SMT), based on joint modeling of word alignment and the topical aspects underlying bilingual document pairs via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this new paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow, to ensure coherence of topical context in the alignment of matching words between languages, during likelihood-based training of topic-dependent translational lexicons, as well as topic representations in each language. The resulting trained HM-BiTAM can not only display topic patterns like other methods such as LDA, but now for bilingual corpora; it also offers a principled way of inferring optimal translation in a context-dependent way. Our method integrates the conventional IBM Models based on HMM --- a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model, and we report an extensive empirical analysis (in many way complementary to the description-oriented of our method in three aspects: word alignment, bilingual topic representation, and translation.",
        "authors": "Bing Zhao, Eric P. Xing",
        "pdf": "http://papers.nips.cc/paper/3365-hm-bitam-bilingual-topic-exploration-word-alignment-and-translation.pdf",
        "title": "HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation"
    },
    {
        "abstract": "We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods.",
        "authors": "Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun",
        "pdf": "http://papers.nips.cc/paper/3305-a-general-boosting-method-and-its-application-to-learning-ranking-functions-for-web-search.pdf",
        "title": "A General Boosting Method and its Application to Learning Ranking Functions for Web Search"
    },
    {
        "abstract": "Reliably recovering 3D human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions. We define priors for people tracking using a Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable models---definining a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction---with those of spectral manifold learning methods---no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements.",
        "authors": "Zhengdong Lu, Cristian Sminchisescu, Miguel Á. Carreira-Perpiñán",
        "pdf": "http://papers.nips.cc/paper/3226-people-tracking-with-the-laplacian-eigenmaps-latent-variable-model.pdf",
        "title": "People Tracking with the Laplacian Eigenmaps Latent Variable Model"
    },
    {
        "abstract": "Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data. In this paper we study a variant of this problem where the original n input variables are compressed by a random linear transformation to m \\ll n examples in p dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for \\ell_1-regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ``sparsistence.'' In addition, we show that \\ell_1-regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ``persistence.'' Finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero.",
        "authors": "Shuheng Zhou, Larry Wasserman, John D. Lafferty",
        "pdf": "http://papers.nips.cc/paper/3280-compressed-regression.pdf",
        "title": "Compressed Regression"
    },
    {
        "abstract": "It is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements. We assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrix-variate t model (MVTM) to predict those missing elements. We show that MVTM generalizes a range of known probabilistic models, and automatically performs model selection to encourage sparse predictive models. Due to the non-conjugacy of its prior, it is difficult to make predictions by computing the mode or mean of the posterior distribution. We suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood, which is very efficient and scalable. The experiments on a toy data and EachMovie dataset show a good predictive accuracy of the model.",
        "authors": "Shenghuo Zhu, Kai Yu, Yihong Gong",
        "pdf": "http://papers.nips.cc/paper/3203-predictive-matrix-variate-t-models.pdf",
        "title": "Predictive Matrix-Variate t Models"
    },
    {
        "abstract": "Abstract Missing",
        "authors": "Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione",
        "pdf": "http://papers.nips.cc/paper/3306-regret-minimization-in-games-with-incomplete-information.pdf",
        "title": "Regret Minimization in Games with Incomplete Information"
    }
]